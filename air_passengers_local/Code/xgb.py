import osimport importlibimport numpy as npimport pandas as pdimport matplotlib.pyplot as pltfrom sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScalerfrom sklearn.compose import make_column_transformerfrom sklearn.ensemble import RandomForestRegressorfrom sklearn.pipeline import make_pipelinefrom sklearn.model_selection import cross_val_scoreimport problemfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCVfrom sklearn.impute import SimpleImputerfrom sklearn.preprocessing import FunctionTransformerfrom sklearn.inspection import permutation_importancefrom sklearn.model_selection import train_test_splitfrom sklearn.svm import SVRimport xgboost as xgbdef _merge_external_data(X):    filepath = os.path.join(        os.path.dirname(__file__), 'external_data.csv'    )    # Make sure that DateOfDeparture is of dtype datetime    X = X.copy()  # modify a copy of X    X.loc[:, "DateOfDeparture"] = pd.to_datetime(X['DateOfDeparture'])    # Parse date to also be of dtype datetime    data_weather = pd.read_csv(filepath, parse_dates=["DateOfDeparture"])    X_weather = data_weather[['DateOfDeparture', 'Arrival',                              'Max TemperatureC', 'Mean VisibilityKm', 'holidays']]    X_merged = pd.merge(        X, X_weather, how='left', on=['DateOfDeparture', 'Arrival'], sort=False    )    return X_mergeddef _encode_dates(X):    # Make sure that DateOfDeparture is of dtype datetime    X = X.copy()  # modify a copy of X    X.loc[:, "DateOfDeparture"] = pd.to_datetime(X['DateOfDeparture'])    # Encode the date information from the DateOfDeparture columns    X.loc[:, 'year'] = X['DateOfDeparture'].dt.year    X.loc[:, 'month'] = X['DateOfDeparture'].dt.month    X.loc[:, 'day'] = X['DateOfDeparture'].dt.day    X.loc[:, 'weekday'] = X['DateOfDeparture'].dt.weekday    X.loc[:, 'week'] = X['DateOfDeparture'].dt.week    X.loc[:, 'n_days'] = X['DateOfDeparture'].apply(        lambda date: (date - pd.to_datetime("1970-01-01")).days    )    # Finally we can drop the original columns from the dataframe    return X.drop(columns=["DateOfDeparture"])#Loading  training dataX, y = problem.get_train_data()#########################################################################################################################                                            #Xgboost with OneHotEncoder and StandardScaler######################################################################################################################################################################Preprocessing training data#####################################data_merger = FunctionTransformer(_merge_external_data)date_encoder = FunctionTransformer(_encode_dates)date_cols = ["DateOfDeparture"]categorical_encoder = make_pipeline(    SimpleImputer(strategy="constant", fill_value="missing"),    OneHotEncoder(handle_unknown="ignore"))categorical_cols = [    "Arrival", "Departure", "day", "weekday", "holidays", "week", "n_days"]preprocessor = make_column_transformer(    (categorical_encoder, categorical_cols))#############################################Find best parameters#############################################"""parameters = {"xgbregressor__learning_rate"    : [ 0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,              "xgbregressor__max_depth"        : [ 3,5,8, 10, 12, 15, 17],              "xgbregressor__min_child_weight" : [ 1, 3, 5, 7, 9 ],              "xgbregressor__gamma"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],              "xgbregressor__colsample_bytree" : [ 0.4, 0.5 , 0.7,0.9, 1] }pipeline_cv = make_pipeline(data_merger,date_encoder,preprocessor, xgb.XGBRegressor())cv_xgb = RandomizedSearchCV(pipeline_cv, parameters, cv=5, n_jobs=4, n_iter = 10)cv_xgb.fit(X, y)best_parameters = cv_xgb.best_params_print(cv_xgb.best_score_)"""#############################################Create pipeline with best parameters###############################Best parameterslearning_rate = 0.1max_depth = 12gamma = 0colsample_bytree = 0.8min_child_weight = 5subsample = 0.9n_estimators=400regressor = xgb.XGBRegressor(n_estimators = n_estimators, learning_rate = learning_rate, max_depth = max_depth, gamma=gamma,colsample_bytree = colsample_bytree, min_child_weight = min_child_weight)pipeline = make_pipeline(data_merger, date_encoder,preprocessor, regressor)scores = cross_val_score(    pipeline, X, y, cv=5, scoring='neg_mean_squared_error')rmse_scores = np.sqrt(-scores)print(    f"RMSE: {np.mean(rmse_scores):.4f} +/- {np.std(rmse_scores):.4f}")#RMSE: 0.3749 +/- 0.0197