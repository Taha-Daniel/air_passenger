{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a href=\"http://www.datascience-paris-saclay.fr\">Paris Saclay Center for Data Science</a>\n",
    "# <a href=https://www.ramp.studio/problems/air_passengers>RAMP</a> on predicting the number of air passengers\n",
    "\n",
    "<i> Balázs Kégl (LAL/CNRS), Alex Gramfort (LTCI/Telecom ParisTech), Djalel Benbouzid (UPMC), Mehdi Cherti (LAL/CNRS) </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The data set was donated to us by an unnamed company handling flight ticket reservations. The data is thin, it contains\n",
    "<ul>\n",
    "<li> the date of departure\n",
    "<li> the departure airport\n",
    "<li> the arrival airport\n",
    "<li> the mean and standard deviation of the number of weeks of the reservations made before the departure date\n",
    "<li> a field called <code>log_PAX</code> which is related to the number of passengers (the actual number were changed for privacy reasons)\n",
    "</ul>\n",
    "\n",
    "The goal is to predict the <code>log_PAX</code> column. The prediction quality is measured by RMSE. \n",
    "\n",
    "The data is obviously limited, but since data and location informations are available, it can be joined to external data sets. <b>The challenge in this RAMP is to find good data that can be correlated to flight traffic</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from rampwf.utils.importing import import_module_from_source\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U seaborn  # if you don't have it, or pip3 for python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch the data and load it in pandas\n",
    "\n",
    "First we load `problem.py` that parameterizes the challenge. It contains some objects taken off the shelf from `ramp-workflow` (e.g., `Predictions` type, scores, and data reader). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = import_module_from_source('problem.py', 'problem')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_train_data` loads the training data and returns an `pandas` object (input) and a `np.array` object (output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.read_csv('data/train.csv.bz2')\n",
    "\n",
    "X_df = dat.drop(columns='log_PAX')\n",
    "\n",
    "y_array = dat['log_PAX'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df, y_array = problem.get_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(min(X_df['DateOfDeparture']))\n",
    "print(max(X_df['DateOfDeparture']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df['Departure'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(y_array, bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df.hist('std_wtd', bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df.hist('WeeksToDeparture', bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_array.mean())\n",
    "print(y_array.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting dates into numerical columns is a common operation when time series data is analyzed with non-parametric predictors. The code below makes the following transformations:\n",
    "\n",
    "- numerical columns for year (2011-2012), month (1-12), day of the month (1-31), day of the week (0-6), and week of the year (1-52)\n",
    "- number of days since 1970-01-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_encoded = X_df\n",
    "\n",
    "# following http://stackoverflow.com/questions/16453644/regression-with-date-variable-using-scikit-learn\n",
    "X_encoded['DateOfDeparture'] = pd.to_datetime(X_encoded['DateOfDeparture'])\n",
    "X_encoded['year'] = X_encoded['DateOfDeparture'].dt.year\n",
    "X_encoded['month'] = X_encoded['DateOfDeparture'].dt.month\n",
    "X_encoded['day'] = X_encoded['DateOfDeparture'].dt.day\n",
    "X_encoded['weekday'] = X_encoded['DateOfDeparture'].dt.weekday\n",
    "X_encoded['week'] = X_encoded['DateOfDeparture'].dt.week\n",
    "X_encoded['n_days'] = X_encoded['DateOfDeparture'].apply(lambda date: (date - pd.to_datetime(\"1970-01-01\")).days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_encoded.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform all preprocessing steps within a scikit-learn [pipeline](https://scikit-learn.org/stable/modules/compose.html) which chains together tranformation and estimator steps. This offers offers convenience and safety (help avoid leaking statistics from your test data into the trained model in cross-validation) and the whole pipeline can be evaluated with `cross_val_score`.\n",
    "\n",
    "To perform the above encoding within a scikit-learn [pipeline](https://scikit-learn.org/stable/modules/compose.html) we will a function and using `FunctionTransformer` to make it compatible with scikit-learn API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "def _encode_dates(X):\n",
    "\n",
    "    # to avoid SettingwithCopyWarning\n",
    "    X_encoded = X.copy()\n",
    "    X_encoded.loc[:, 'year'] = X_encoded['DateOfDeparture'].dt.year\n",
    "    X_encoded.loc[:, 'month'] = X_encoded['DateOfDeparture'].dt.month\n",
    "    X_encoded.loc[:, 'day'] = X_encoded['DateOfDeparture'].dt.day\n",
    "    X_encoded.loc[:, 'weekday'] = X_encoded['DateOfDeparture'].dt.weekday\n",
    "    X_encoded.loc[:, 'week'] = X_encoded['DateOfDeparture'].dt.week\n",
    "    X_encoded.loc[:, 'n_days'] = X_encoded['DateOfDeparture'].apply(\n",
    "        lambda date: (date - pd.to_datetime(\"1970-01-01\")).days\n",
    "    )\n",
    "    \n",
    "    return X_encoded\n",
    "\n",
    "date_transformer = FunctionTransformer(_encode_dates, validate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regressor\n",
    "\n",
    "Preprocessing for linear regressor includes one-hot encoding non-ordinal categorical variables. We will use\n",
    "[`make_column_transformer`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.make_column_transformer.html) to [`OneHotEncode`](https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-categorical-features) the categorical variables and drop the `DateOfDeparture` feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "categorical_cols = ['Departure', 'Arrival', 'year', 'month', 'day', 'weekday', 'week']\n",
    "drop_cols = ['DateOfDeparture']\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (OneHotEncoder(handle_unknown='ignore'), categorical_cols),\n",
    "    ('drop', drop_cols),\n",
    "    remainder='passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now combine our `preprocessor` with the `LinearRegression` estimator in a `Pipeline`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    date_transformer, preprocessor, LinearRegression()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can cross-validate our `pipeline` using `cross_val_score`. Below we will have specified `cv=5` meaning KFold cross-valdiation splitting will be used, with 8 folds. The mean squared error regression loss is calculated for each split. The output `score` will be an array of 5 scores from each KFold. The score mean and standard deviation of the 5 scores is printed at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(pipeline, X_df, y_array, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "print(\"RMSE: {:.4f} +/- {:.4f}\".format(\n",
    "    np.mean(np.sqrt(-scores)), np.std(np.sqrt(-scores))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding of our categorical features is not required, nor recommended for tree based estimators so we will amend our `preprocessor` to use `OrdinalEncoder` instead.\n",
    "\n",
    "Technically, the numerical values of the date columns (e.g., `month`, `day`) are already ordinally encoded and performing `OrdinalEncoder` is simply a style preference as it will map the values starting from 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "categorical_cols = ['Departure', 'Arrival', 'year', 'month', 'day', 'weekday', 'week']\n",
    "drop_cols = ['DateOfDeparture']\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (OrdinalEncoder(), categorical_cols),\n",
    "    ('drop', drop_cols),\n",
    "    remainder='passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "n_estimators = 10\n",
    "max_depth = 10\n",
    "max_features = 10\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    date_transformer, preprocessor,\n",
    "    RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, max_features=max_features)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(pipeline, X_df, y_array, cv=5, scoring='neg_mean_squared_error')\n",
    "print(\"RMSE: {:.4f} +/- {:.4f}\".format(\n",
    "    np.mean(np.sqrt(-scores)), np.std(np.sqrt(-scores))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging external data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective in this RAMP data challenge is to find good data that can be correlated to flight traffic. We will use some weather data (saved in `submissions/starting_kit`) to provide an example of how to merge external data in a scikit-learn pipeline.\n",
    "\n",
    "Your external data will need to be included in your submissions folder - see [RAMP submissions](#RAMP-submissions) for more details.\n",
    "\n",
    "First we will define a function that merges the external data to our feature data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " filepath = os.path.join(os.path.dirname(__file__),\n",
    "                            'external_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need this because the global variable __file__ (the path of the current file)\n",
    "# does not exist if we are in a notebook\n",
    "__file__ = 'submissions/starting_kit'\n",
    "\n",
    "def _merge_external_data(X):\n",
    "    filepath = os.path.join(os.path.dirname(__file__),\n",
    "                            'external_data.csv')\n",
    "    data_weather = pd.read_csv(filepath)\n",
    "    X_weather = data_weather[['Date', 'AirPort', 'Max TemperatureC']]\n",
    "    X_weather = X_weather.rename(\n",
    "        columns={'Date': 'DateOfDeparture', 'AirPort': 'Arrival'})\n",
    "    X_merged = pd.merge(\n",
    "        X, X_weather, how='left', on=['DateOfDeparture', 'Arrival'], sort=False\n",
    "    )\n",
    "    return X_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double check that our function works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_merge_external_data(X_df).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `FunctionTransformer` to make our function compatible with scikit-learn API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_transformer = FunctionTransformer(_merge_external_data, validate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now assemble our pipeline using the same `date_transformer` and `preprocessor` as above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 10\n",
    "max_depth = 10\n",
    "max_features = 10\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    merge_transformer, date_transformer, preprocessor,\n",
    "    RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, max_features=max_features)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(pipeline, X_df, y_array, cv=5, scoring='neg_mean_squared_error')\n",
    "print(\"RMSE: {:.4f} +/- {:.4f}\".format(\n",
    "    np.mean(np.sqrt(-scores)), np.std(np.sqrt(-scores))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable importances\n",
    "\n",
    "To investigate the relative importance of each feature we can use the `feature_importances_` method of the final estimator after we `fit` it.\n",
    "\n",
    "Note that `reg[-1]` subsets the final tuple ('step') of the pipeline and `reg[-1][1]` subsets the second item (the `RandomForestRegressor`) of the final tuple.\n",
    "\n",
    "Below we will plot feature importance, ordered from most important to least important:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = pipeline.fit(X_df, y_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "ordering = np.argsort(reg[-1][1].feature_importances_)[::-1]\n",
    "\n",
    "importances = reg[-1][1].feature_importances_[ordering]\n",
    "feature_names = X_columns[ordering]\n",
    "\n",
    "x = np.arange(len(feature_names))\n",
    "plt.bar(x, importances)\n",
    "plt.xticks(x - 0.2, feature_names, rotation=45, fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAMP submissions\n",
    "\n",
    "For submitting to the [RAMP site](http://ramp.studio), you will need to create a `estimator.py` file that defines a `get_estimator` function which returns a scikit-learn [pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html).\n",
    "\n",
    "For example, to submit our last example above, we would define our `pipeline` within the function and return the pipeline at the end. Remember to include all the necessary imports at the beginning of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "def _merge_external_data(X):\n",
    "    X.loc[:, 'DateOfDeparture'] = pd.to_datetime(X.loc[:, 'DateOfDeparture'])\n",
    "    filepath = os.path.join(\n",
    "        os.path.dirname(__file__), 'external_data.csv'\n",
    "    )\n",
    "    data_weather = pd.read_csv(filepath, parse_dates=[\"Date\"])\n",
    "    X_weather = data_weather[['Date', 'AirPort', 'Max TemperatureC']]\n",
    "    X_weather = X_weather.rename(\n",
    "        columns={'Date': 'DateOfDeparture', 'AirPort': 'Arrival'}\n",
    "    )\n",
    "    X_merged = pd.merge(\n",
    "        X, X_weather, how='left', on=['DateOfDeparture', 'Arrival'], sort=False\n",
    "    )\n",
    "    return X_merged\n",
    "\n",
    "\n",
    "def _encode_dates(X):\n",
    "    X.loc[:, 'year'] = X['DateOfDeparture'].dt.year\n",
    "    X.loc[:, 'month'] = X['DateOfDeparture'].dt.month\n",
    "    X.loc[:, 'day'] = X['DateOfDeparture'].dt.day\n",
    "    X.loc[:, 'weekday'] = X['DateOfDeparture'].dt.weekday\n",
    "    X.loc[:, 'week'] = X['DateOfDeparture'].dt.week\n",
    "    X.loc[:, 'n_days'] = X['DateOfDeparture'].apply(\n",
    "        lambda date: (date - pd.to_datetime(\"1970-01-01\")).days\n",
    "    )\n",
    "    return X\n",
    "\n",
    "\n",
    "def get_estimator():\n",
    "    merge_transformer = FunctionTransformer(_merge_external_data)\n",
    "    date_transformer = FunctionTransformer(_encode_dates)\n",
    "\n",
    "    categorical_cols = ['Arrival', 'Departure']\n",
    "    drop_col = ['DateOfDeparture']\n",
    "    preprocessor = make_column_transformer(\n",
    "        (OrdinalEncoder(), categorical_cols),\n",
    "        ('drop', drop_col),\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "    pipeline = make_pipeline(\n",
    "        merge_transformer, date_transformer, preprocessor,\n",
    "        RandomForestRegressor(n_estimators=10, max_depth=10, max_features=10)\n",
    "    )\n",
    "    return pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you take a look at the sample submission in the directory `submissions/starting_kit`, you will find a file named `estimator.py`, which includes the same code as above.\n",
    "\n",
    "You can test that the sample submission works by running `ramp_test_submission` in your terminal (ensure that `ramp-workflow` has been installed and you are in the `air_passenger` ramp kit directory). Alternatively, you can also run `ramp_test_submission` with a notebook by adding `!` at the beginning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ramp_test_submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a new submission you can either amend the files within `submissions/starting_kit` or create a new folder within `submissions`, naming it anythin you like, and save our `estimator.py` file within the new folder.\n",
    "\n",
    "### External data\n",
    "\n",
    "Your external data needs to be saved with your `estimator.py`. For example the sample suubmission directory (`submissions/starting_kit`) contains a `estimator.py` file and the external data file `external_data_mini.csv`. Ensure that you amend your code to match the name of the external data file.\n",
    "\n",
    "### Local testing\n",
    "\n",
    "You can test that your submission works locally by running:\n",
    "\n",
    "`ramp_test_submission --submission <folder>`\n",
    "\n",
    "where `<folder>` is the name of the new folder you created, within `submissions`.\n",
    "\n",
    "It is <b><span style=\"color:red\">important that you test your submission files before submitting them</span></b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting to [ramp.studio](http://ramp.studio)\n",
    "\n",
    "Once you found a good solution, you can submit it to [ramp.studio](http://www.ramp.studio). First, if it is your first time using RAMP, [sign up](http://www.ramp.studio/sign_up), otherwise [log in](http://www.ramp.studio/login). Then, find the appropriate open event for the [titanic](http://www.ramp.studio/events/air_passengers) challenge. Sign up for the event. Note that both RAMP and event signups are controled by RAMP administrators, so there **can be a delay between asking for signup and being able to submit**.\n",
    "\n",
    "Once your signup request(s) have been accepted, you can go to your [sandbox](http://www.ramp.studio/events/air_passengers/sandbox) and copy-paste (or upload) your `submissions.py` file. Save your submission, name it, then click 'submit'. The submission is trained and tested on our backend in the same way as `ramp_test_submission` does it locally. While your submission is waiting in the queue and being trained, you can find it in the \"New submissions (pending training)\" table in [my submissions](http://www.ramp.studio/events/air_passengers/my_submissions). Once it is trained, you get a mail, and your submission shows up on the [public leaderboard](http://www.ramp.studio/events/air_passengers/leaderboard).\n",
    "\n",
    "If there is an error (despite having tested your submission locally with `ramp_test_submission`), it will show up in the \"Failed submissions\" table in [my submissions](http://www.ramp.studio/events/air_passengers/my_submissions). You can click on the error to see part of the trace.\n",
    "\n",
    "After submission, do not forget to give credits to the previous submissions you reused or integrated into your submission.\n",
    "\n",
    "The data set we use at the backend is usually different from what you find in the starting kit, so the score may be different.\n",
    "\n",
    "The usual way to work with RAMP is to explore solutions, add feature transformations, select models, perhaps do some AutoML/hyperopt, etc., _locally_, and checking them with `ramp_test_submission`. The script prints mean cross-validation scores \n",
    "```\n",
    "----------------------------\n",
    "train rmse = 0.748 ± 0.0117\n",
    "valid rmse = 0.858 ± 0.0111\n",
    "test rmse = 0.881 ± 0.005\n",
    "```\n",
    "The official score in this RAMP (the first score column after \"historical contributivity\" on the leaderboard is root mean squared error (\"rmse\"), so the line that is relevant in the output of `ramp_test_submission` is `valid rmse = 0.858 ± 0.0111`. When the score is good enough, you can submit it at the RAMP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More information\n",
    "\n",
    "You can find more information in the [README](https://github.com/paris-saclay-cds/ramp-workflow/blob/master/README.md) of the [ramp-workflow library](https://github.com/paris-saclay-cds/ramp-workflow)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contact\n",
    "\n",
    "Don't hesitate to [contact us](mailto:admin@ramp.studio?subject=air passengers notebook)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
